{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas to handle the files.\n",
    "#First read the list file and converting them to a list of dataframes using \n",
    "#Add them all into one big frame using concat\n",
    "corpus=['amazon_cells_labelled.txt',\"imdb_labelled.txt\",\"yelp_labelled.txt\"]\n",
    "data = pd.concat(pd.read_csv(file, sep='\\t', names=['Text','Label'], index_col= None, header = 0 )\n",
    "for file in corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalised form of NaiveBayes Classifier\n",
    "#Training\n",
    "def NaiveBayes_A():\n",
    "    #noramalisation\n",
    "    stop_words =set(stopwords.words(\"english\"))\n",
    "    \n",
    "    #In order to count the word count per text it is easier to use Term Frequency Inverse Document Frequency\n",
    "    #Tfid transforms text to feature vecors,\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, strip_accents ='ascii', stop_words = stop_words, lowercase=True)\n",
    "    \n",
    "    #Setting labels where 0 is negative and 1 is positive\n",
    "    y = data.Label\n",
    "    x= vectorizer.fit_transform(data.Text) #Converting the dataframes in the data into features.\n",
    "    \n",
    "    #splitting our data into training and test sets\n",
    "    x_train, x_test, y_train, y_test =train_test_split(x,y, random_state = 35 , train_size =0.95, test_size =0.05)\n",
    "    \n",
    "    #Training the Classifier (MultinomialNB Normalised Classifier(MNBN Classifier))\n",
    "    MNBN_classifier = naive_bayes.MultinomialNB()\n",
    "    model = MNBN_classifier.fit(x_train, y_train)\n",
    "   \n",
    "\n",
    "    return MNBN_classifier , vectorizer, model, x_test, y_test\n",
    "    \n",
    "#NaiveBayes_A()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE NORMALISED NAIVE BAIYES CLASSIFIER\n",
    "def testNaiveBayes_A(file):\n",
    "    #Reading of the file\n",
    "    df = pd.read_csv(file, sep='\\t', names=['Text','Label'], index_col=None, header =-1)\n",
    "    \n",
    "    #Training on the normalised nb classifier\n",
    "    MNBN_classifier , vectorizer, model, x_test, y_test = NaiveBayes_A()\n",
    "    \n",
    "    ##Setting labels where 0 is negative and 1 is positive\n",
    "    y = list(df.Label)\n",
    "    x=  df.Text\n",
    "\n",
    "\n",
    "    #Making sentiment classification\n",
    "    predictL =[]\n",
    "    for i in range (len(x)):\n",
    "        sentiment = np.array([str(x[i])])\n",
    "        sentiment_transform = vectorizer.transform(sentiment)\n",
    "        predict =   MNBN_classifier.predict(sentiment_transform)\n",
    "        predictL.append(predict[0])\n",
    "\n",
    "        \n",
    "   #Evaluating the model using the classification report function form sklearn\n",
    "\n",
    "    predicted = model.predict(x_test)\n",
    "    print(\"Our Accuracy is: \" ,np.mean(predicted == y_test)*100) \n",
    "    \n",
    "    print(\" Accuracy is: \" ,accuracy_score(y_test, predicted)*100) \n",
    "    \n",
    "    print(\" Below is how the classifier was evaluated: \\n\" ,classification_report(y_test, predicted))\n",
    "    \n",
    "    return(predictL,\"\\n\")\n",
    "#testNaiveBayes_A(\"yelp_labelled.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnNormalised form of NaiveBayes Classifier\n",
    "#Training\n",
    "def NaiveBayes_B():\n",
    "\n",
    "    #In order to count the word count per text it is easier to use Term Frequency Inverse Document Frequency\n",
    "    #Tfid transforms text to feature vecors,\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, strip_accents =None, lowercase=False)\n",
    "    \n",
    "    #Setting labels where 0 is negative and 1 is positive\n",
    "    y = data.Label\n",
    "    x= vectorizer.fit_transform(data.Text) #Converting the dataframes in the data into features.\n",
    "    \n",
    "    #splitting our data into training and test sets\n",
    "    x_train, x_test, y_train, y_test =train_test_split(x,y, random_state = 35 , train_size =0.95, test_size =0.05)\n",
    "    \n",
    " #Training the Classifier (MultinomialNB Unormalised Classifier(MNBU Classifier))\n",
    "\n",
    "    MNBU_classifier = naive_bayes.MultinomialNB()\n",
    "    model= MNBU_classifier.fit(x_train, y_train)\n",
    "   \n",
    "\n",
    "    return MNBU_classifier , vectorizer, model, x_test,y_test\n",
    "    \n",
    "#NaiveBayes_B()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TRAINING THE UNNORMALISED NAIVE BAIYES CLASSIFIER\n",
    "def testNaiveBayes_B(file):\n",
    "    #Reading of the file\n",
    "    df = pd.read_csv(file, sep='\\t', names=['Text','Label'], index_col=None, header =-1)\n",
    "    \n",
    "      #Training on the unnormalised nb classifier\n",
    "    MNBU_classifier  , vectorizer, model, x_test, y_test = NaiveBayes_B()\n",
    "    ##Setting labels where 0 is negative and 1 is positive\n",
    "    y = list(df.Label)\n",
    "    x=  df.Text\n",
    "    \n",
    "    #Making sentiment classification\n",
    "    predictL =[]\n",
    "    for i in range (len(x)):\n",
    "        sentiment = np.array([str(x[i])])\n",
    "        sentiment_transform = vectorizer.transform(sentiment)\n",
    "        predict =   MNBU_classifier.predict(sentiment_transform)\n",
    "        predictL.append(predict[0])\n",
    "        \n",
    "    \n",
    "    #Evaluating the model using the classification report function form sklearn\n",
    "    predicted = model.predict(x_test)\n",
    "    print(\"Our Accuracy is: \" ,np.mean(predicted == y_test)*100) \n",
    "    \n",
    "    #print(\" Accuracy is: \" ,accuracy_score(y_test, predicted)*100) \n",
    "    \n",
    "    print(\" Below is how the classifier is evaluated: \\n\" ,classification_report(y_test, predicted))\n",
    "    return(predictL,\"\\n\")\n",
    "    \n",
    "    \n",
    "#testNaiveBayes_B(\"yelp_labelled.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A NORMALISED LOGISTIC REGRESSION TRAIN FUNCTION.\n",
    "def normLR_A():\n",
    "    #noramalisation\n",
    "    stop_words =set(stopwords.words(\"english\"))\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, strip_accents ='ascii', stop_words = stop_words, lowercase=True)\n",
    "    \n",
    "    #Setting labels where 0 is negative and 1 is positive\n",
    "     \n",
    "    y = data.Label\n",
    "    x=vectorizer.fit_transform(data.Text)# transfroming the data to features \n",
    "    \n",
    "    #Removing unnecessary punctuations\n",
    "    df['Text'] = df.Text.str.replace('[^\\w\\s]', '') \n",
    " \n",
    "    #Training the data\n",
    "    x_train, x_test, y_train, y_test =train_test_split(x,y, random_state = 40, train_size =0.95, test_size =0.05)\n",
    "   \n",
    "    #Training the Classifier (Logistic Regression Normalised Classifier(LRN))\n",
    "    LRN_classifier = LogisticRegression()\n",
    "    model = LRN_classifier.fit(x_train, y_train)\n",
    "        \n",
    "    return LRN_classifier, vectorizer, model, x_test, y_test\n",
    "#normLR_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE NORMALISED LOGISTIC REGRESSION CLASSIFIER\n",
    "def testNormLR_A(file):\n",
    "    #Reading of the file\n",
    "    df = pd.read_csv(file, sep='\\t', names=['Text','Label'], index_col=None, header =-1)\n",
    "    \n",
    "      #Training on the normalised LR classifier\n",
    "    LRN_classifier , vectorizer, model, x_test, y_test = normLR_A()\n",
    "    ##Setting labels where 0 is negative and 1 is positive\n",
    "    y = list(df.Label)\n",
    "    x=  df.Text\n",
    "\n",
    "    #Making sentiment classification\n",
    "    predictL =[]\n",
    "    for i in range (len(x)):\n",
    "        sentiment = np.array([str(x[i])])\n",
    "        sentiment_transform = vectorizer.transform(sentiment)\n",
    "        predict =  LRN_classifier .predict(sentiment_transform)\n",
    "        predictL.append(predict[0])\n",
    "        \n",
    "    #Evaluating the model using the classification report function form sklearn\n",
    "    predicted = model.predict(x_test)\n",
    "    print(\"Our Accuracy is: \" ,np.mean(predicted == y_test)*100) \n",
    "    \n",
    "    print(\" Accuracy is: \" , accuracy_score (y_test, predicted)*100) \n",
    "    \n",
    "    print(\" Below is how the classifier is evaluated: \\n\" ,classification_report(y_test, predicted))\n",
    "    return(predictL,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A-Sackey\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=False,\n",
       "         vocabulary=None),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " <138x5883 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1631 stored elements in Compressed Sparse Row format>,\n",
       " 810    1\n",
       " 688    1\n",
       " 993    0\n",
       " 480    1\n",
       " 956    0\n",
       " 593    1\n",
       " 996    0\n",
       " 34     0\n",
       " 328    0\n",
       " 67     0\n",
       " 644    0\n",
       " 243    0\n",
       " 374    0\n",
       " 578    1\n",
       " 711    1\n",
       " 468    1\n",
       " 10     1\n",
       " 689    1\n",
       " 142    0\n",
       " 556    1\n",
       " 337    0\n",
       " 192    0\n",
       " 746    1\n",
       " 717    1\n",
       " 964    0\n",
       " 229    1\n",
       " 174    1\n",
       " 605    1\n",
       " 817    0\n",
       " 259    0\n",
       "       ..\n",
       " 551    1\n",
       " 517    1\n",
       " 932    0\n",
       " 322    0\n",
       " 272    1\n",
       " 448    0\n",
       " 628    1\n",
       " 914    0\n",
       " 42     0\n",
       " 695    1\n",
       " 638    0\n",
       " 36     1\n",
       " 804    0\n",
       " 395    0\n",
       " 988    0\n",
       " 119    0\n",
       " 401    1\n",
       " 121    1\n",
       " 571    1\n",
       " 180    0\n",
       " 209    0\n",
       " 794    1\n",
       " 765    1\n",
       " 410    1\n",
       " 836    0\n",
       " 443    1\n",
       " 148    0\n",
       " 144    1\n",
       " 159    1\n",
       " 963    0\n",
       " Name: Label, Length: 138, dtype: int64)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AN UNNORMALISED  LOGISTIC REGRESSION TRAIN FUNCTION.\n",
    "def unNormLR_B():\n",
    "    \n",
    "    #making an unnormalised form of our classifier y removing stop words, and not performing any form of normalisation\n",
    "    UnNormalisedVectorizer = TfidfVectorizer(use_idf=False, strip_accents=None, lowercase=False )\n",
    "    \n",
    "    #Setting labels where 0 is negative and 1 is positive\n",
    "    y = data.Label\n",
    "    x= UnNormalisedVectorizer.fit_transform(data.Text)# transfroming the data to features \n",
    "    \n",
    "    #Training the data\n",
    "    x_train, x_test, y_train, y_test =train_test_split(x,y, random_state = 40, train_size =0.95, test_size =0.05)\n",
    "    \n",
    "    #Training the Classifier ( UnNormalised Logistic Regression Classifier)\n",
    "    UnNormalisedLr_classifier =LogisticRegression()\n",
    "    model = UnNormalisedLr_classifier.fit(x_train, y_train)\n",
    "    \n",
    "            \n",
    "    return UnNormalisedLr_classifier, UnNormalisedVectorizer, model, x_test, y_test\n",
    "unNormLR_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  Label\n",
      "0                This GPS tracker works like a charm.    NaN\n",
      "1   When I opened the box the product was not in t...    NaN\n",
      "2        Everyone should have one who owns a computer    NaN\n",
      "3                                  Buy something else    NaN\n",
      "4   Pure junk do not buy ever the greatest load of...    NaN\n",
      "5      The DataVac was used and full of dust and dirt    NaN\n",
      "6   Not so great...bought to clean the bobbin case...    NaN\n",
      "7   It is a great size, I keep it in my desk drawe...    NaN\n",
      "8   I just bought this Vacuum. It's just good for ...    NaN\n",
      "9   This is just perfect for vacuuming out the lin...    NaN\n",
      "10  I use it mostly to vacuum threads on the sewin...    NaN\n",
      "11  I have found this mini vac. to be everything i...    NaN\n",
      "12  I ordered the Pork Prime Rib Chop it was beaut...    NaN\n",
      "13  A bastion of fine dining in The City for 20 ye...    NaN\n",
      "14  Took my brand new bmw in for service. When I p...    NaN\n",
      "15  I'll never buy another car from this location ...    NaN\n",
      "16  Service Department, once the crown jewel of th...    NaN\n",
      "17  Kevin is very friendly, accommodating and has ...    NaN\n",
      "18  Had a great experience leasing a new car here ...    NaN\n",
      "19  Following the showing we were all dissapointed...    NaN\n",
      "20                       Shame I can't rate this 0/10    NaN\n",
      "21  This movie has it all, Great acting , action, ...    NaN\n",
      "22                               Total waste of time.    NaN\n",
      "23  If this is critically acclaimed and highly rat...    NaN\n",
      "24  Clearly, Black Panther represents a great step...    NaN\n",
      "Our Accuracy is:  73.18840579710145\n",
      " Accuracy is:  73.18840579710145\n",
      " Below is how the classifier is evaluated: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71        64\n",
      "           1       0.75      0.74      0.75        74\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       138\n",
      "   macro avg       0.73      0.73      0.73       138\n",
      "weighted avg       0.73      0.73      0.73       138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A-Sackey\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1],\n",
       " '\\n')"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING THE NORMALISED LOGISTIC REGRESSION CLASSIFIER\n",
    "def testUnNormLR_B(file):\n",
    "    #Reading of the file\n",
    "    df = pd.read_csv(file, sep='\\t', names=['Text','Label'], index_col=None, header =-1)\n",
    "    print(df)\n",
    "    \n",
    "    #Training on the normalised LR classifier\n",
    "    UnNormalisedLr_classifier, vectorizer, model, x_test, y_test = unNormLR_B()\n",
    "    ##Setting labels where 0 is negative and 1 is positive\n",
    "    y = list(df.Label)\n",
    "    x=  df.Text\n",
    "    \n",
    "    #Making sentiment classification\n",
    "    predictL = []\n",
    "    for i in range (len(x)):\n",
    "        sentiment = np.array([str(x[i])])\n",
    "        sentiment_transform = vectorizer.transform(sentiment)\n",
    "        predict = UnNormalisedLr_classifier.predict(sentiment_transform)\n",
    "        predictL.append(predict[0])\n",
    "        \n",
    "        \n",
    "    #Evaluating the model using the classification report function form sklearn\n",
    "    predicted = model.predict(x_test)\n",
    "    \n",
    "    print(\"Our Accuracy is: \" ,np.mean(predicted == y_test)*100) \n",
    "    \n",
    "    print(\" Accuracy is: \" , accuracy_score (y_test, predicted)*100) \n",
    "    \n",
    "    print(\" Below is how the classifier is evaluated: \\n\" ,classification_report(y_test, predicted))\n",
    "    return(predictL, \"\\n\")    \n",
    "   \n",
    "    \n",
    "\n",
    "testUnNormLR_B(\"test_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(classifier, version, predictL):\n",
    "    file_name = open(\"results-\"+classifier+\"-\"+version+\".txt\", w)\n",
    "    #file_name.write(\"Ouput: \"+\"\\n\")\n",
    "    \n",
    "    for label in predictL[0]:\n",
    "        file_name.write(str(label)+\"\\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    classifier =sys.argv[1]\n",
    "    version = sys.argv[2]\n",
    "    file = sys.argv[3]\n",
    "       \n",
    "    if classifier == \"nb\" and version == \"u\":\n",
    "        predictL  = testNaiveBayes_B(file)\n",
    "    elif classifier == \"nb\" and version == \"n\":\n",
    "        predictL = testNaiveBayes_A(file)\n",
    "    elif classifier == \"lr\" and version == \"n\":\n",
    "        predictL = testNormLR_A(file)\n",
    "    elif classifier == \"lr\" and version == \"u\":\n",
    "         predictL = testUnNormLR_B(file)\n",
    "    else:\n",
    "        print(\"Kindly check your syntax or input the right cominations\")\n",
    "        \n",
    "        sys.exit()\n",
    "        \n",
    "    \n",
    "    results(classifier, version,predictL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
